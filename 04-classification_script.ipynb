{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Using Previous Notebooks Use This Notebook to Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports --- All of this may not be vital\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import log_loss, f1_score, fbeta_score, recall_score, precision_score, confusion_matrix\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import *\n",
    "\n",
    "# Homemade functions required\n",
    "from data_prep_functions import *\n",
    "from interpro_scraping import interpro_scraping_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(112, 94) (62, 94) (174, 94)\n",
      "                                         Protein names  frac_aa_A  frac_aa_C  \\\n",
      "0                                        Serum albumin   0.103448   0.057471   \n",
      "1    Haptoglobin (Zonulin) [Cleaved into: Haptoglob...   0.073892   0.029557   \n",
      "2    Immunoglobulin kappa constant (Ig kappa chain ...   0.065421   0.028037   \n",
      "3    Immunoglobulin heavy constant gamma 1 (Ig gamm...   0.039394   0.027273   \n",
      "4    Serotransferrin (Transferrin) (Beta-1 metal-bi...   0.087393   0.057307   \n",
      "..                                                 ...        ...        ...   \n",
      "169  Hemoglobin subunit beta (Beta-globin) (Hemoglo...   0.102041   0.013605   \n",
      "170  Haptoglobin (Zonulin) [Cleaved into: Haptoglob...   0.073892   0.029557   \n",
      "171  Immunoglobulin heavy constant alpha 1 (Ig alph...   0.067989   0.042493   \n",
      "172  Immunoglobulin lambda constant 7 (Ig lambda-7 ...   0.103774   0.028302   \n",
      "173  Complement C4-A (Acidic complement C4) (C3 and...   0.078555   0.016055   \n",
      "\n",
      "     frac_aa_D  frac_aa_E  frac_aa_F  frac_aa_G  frac_aa_H  frac_aa_I  \\\n",
      "0     0.059113   0.101806   0.057471   0.021346   0.026273   0.014778   \n",
      "1     0.061576   0.061576   0.019704   0.076355   0.032020   0.044335   \n",
      "2     0.046729   0.065421   0.037383   0.037383   0.018692   0.009346   \n",
      "3     0.039394   0.051515   0.030303   0.054545   0.027273   0.015152   \n",
      "4     0.064470   0.060172   0.040115   0.074499   0.027221   0.022923   \n",
      "..         ...        ...        ...        ...        ...        ...   \n",
      "169   0.047619   0.054422   0.054422   0.088435   0.061224   0.000000   \n",
      "170   0.061576   0.061576   0.019704   0.076355   0.032020   0.044335   \n",
      "171   0.033994   0.048159   0.031161   0.062323   0.022663   0.008499   \n",
      "172   0.018868   0.066038   0.018868   0.047170   0.018868   0.000000   \n",
      "173   0.049885   0.060206   0.038417   0.072821   0.021789   0.031537   \n",
      "\n",
      "     frac_aa_K  ...  fraction_exposed_exposed_T  fraction_exposed_exposed_V  \\\n",
      "0     0.098522  ...                    0.067647                    0.035294   \n",
      "1     0.086207  ...                    0.029536                    0.054852   \n",
      "2     0.074766  ...                    0.094595                    0.040541   \n",
      "3     0.084848  ...                    0.101322                    0.026432   \n",
      "4     0.083095  ...                    0.047337                    0.026627   \n",
      "..         ...  ...                         ...                         ...   \n",
      "169   0.074830  ...                    0.058824                    0.047059   \n",
      "170   0.086207  ...                    0.029536                    0.054852   \n",
      "171   0.036827  ...                    0.134454                    0.021008   \n",
      "172   0.084906  ...                    0.120000                    0.053333   \n",
      "173   0.044151  ...                    0.056257                    0.025258   \n",
      "\n",
      "     fraction_exposed_exposed_W  fraction_exposed_exposed_Y  \\\n",
      "0                      0.002941                    0.011765   \n",
      "1                      0.004219                    0.037975   \n",
      "2                      0.000000                    0.013514   \n",
      "3                      0.004405                    0.017621   \n",
      "4                      0.000000                    0.005917   \n",
      "..                          ...                         ...   \n",
      "169                    0.011765                    0.011765   \n",
      "170                    0.004219                    0.037975   \n",
      "171                    0.008403                    0.012605   \n",
      "172                    0.000000                    0.013333   \n",
      "173                    0.004592                    0.011481   \n",
      "\n",
      "     nsp_secondary_structure_coil  nsp_secondary_structure_sheet  \\\n",
      "0                           0.286                          0.000   \n",
      "1                           0.559                          0.372   \n",
      "2                           0.383                          0.514   \n",
      "3                           0.485                          0.467   \n",
      "4                           0.474                          0.186   \n",
      "..                            ...                            ...   \n",
      "169                         0.204                          0.000   \n",
      "170                         0.559                          0.372   \n",
      "171                         0.482                          0.459   \n",
      "172                         0.387                          0.491   \n",
      "173                         0.416                          0.372   \n",
      "\n",
      "     nsp_secondary_structure_helix  nsp_disordered  asa_sum_normalized  \\\n",
      "0                            0.714           0.038            0.464794   \n",
      "1                            0.069           0.074            0.491087   \n",
      "2                            0.103           0.009            0.557026   \n",
      "3                            0.048           0.012            0.517634   \n",
      "4                            0.340           0.030            0.419599   \n",
      "..                             ...             ...                 ...   \n",
      "169                          0.796           0.007            0.509931   \n",
      "170                          0.069           0.074            0.491087   \n",
      "171                          0.059           0.014            0.525706   \n",
      "172                          0.123           0.009            0.572862   \n",
      "173                          0.213           0.011            0.403293   \n",
      "\n",
      "     phase_plasma  \n",
      "0               1  \n",
      "1               1  \n",
      "2               1  \n",
      "3               1  \n",
      "4               1  \n",
      "..            ...  \n",
      "169             0  \n",
      "170             0  \n",
      "171             0  \n",
      "172             0  \n",
      "173             0  \n",
      "\n",
      "[174 rows x 94 columns]\n"
     ]
    }
   ],
   "source": [
    "### import data used to train classifiers ###\n",
    "\n",
    "plasma_total_data_names = pd.read_excel(\"data/\"+'gt15_plasma_features_names_biopy_gravy.xlsx', header=0, index_col=0)\n",
    "# gt6_data = pd.read_excel(\"data/\"+'gt6_plasma_features_names_biopy.xlsx', header=0, index_col=0)\n",
    "csf_total_data_names = pd.read_excel(\"data/\"+'gt15_csf_features_names_biopy_gravy.xlsx', header=0,index_col=0)\n",
    "\n",
    "\n",
    "## sort into names and features\n",
    "## <class 'pandas.core.frame.DataFrame'>\n",
    "features_plasma = plasma_total_data_names.copy()\n",
    "features_plasma = features_plasma.drop(['Corona'], axis=1)\n",
    "names_plasma = plasma_total_data_names['Corona'].copy()\n",
    "\n",
    "features_csf = csf_total_data_names.copy()\n",
    "features_csf = features_csf.drop(['Corona'], axis=1) \n",
    "names_csf = csf_total_data_names['Corona'].copy()\n",
    "\n",
    "### create a merged set\n",
    "features_plasma_labeled = features_plasma.copy()\n",
    "features_csf_labeled = features_csf.copy()\n",
    "\n",
    "features_plasma_labeled['phase_plasma'] = 1\n",
    "features_csf_labeled['phase_plasma'] = 0\n",
    "\n",
    "# Modified by Firebird 10/16/2025 to be able to use with python 3.12 \n",
    "#features_merged = features_plasma_labeled.append(features_csf_labeled, ignore_index=True)\n",
    "#names_merged = names_plasma.append(names_csf, ignore_index=True)\n",
    "\n",
    "features_merged = pd.concat([features_plasma_labeled, features_csf_labeled], ignore_index=True)\n",
    "names_merged = pd.concat([names_plasma, names_csf], ignore_index=True)\n",
    "\n",
    "# QC test prints\n",
    "#print(features_plasma_labeled.shape, features_csf_labeled.shape, features_merged.shape)\n",
    "print(features_merged)#[\"phase_plasma\"] = 1)\n",
    "\n",
    "# set with no phase labeling names are identical to names merged\n",
    "features_merged_naive = features_merged.drop(['phase_plasma'], axis=1)\n",
    "\n",
    "# print(plasma_total_data_names.shape, csf_total_data_names.shape, features_test.shape) ## in case you need to see shapes\n",
    "\n",
    "\n",
    "## there is a known error here, sometimes there is an Unnamed column just drop it code is available in a \n",
    "#lower cell (scaling cell), its a holdover from two merged set\n",
    "\n",
    "# tf_data = features_merged_naive.copy()\n",
    "# tf_data['names'] = names_merged.copy()\n",
    "# tf_data.to_excel('data_for_tensorflow.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### revisions \n",
    "\n",
    "total_data_for_reductions = features_merged_naive.copy()\n",
    "total_data_for_reductions['Corona'] = names_merged\n",
    "\n",
    "total_data_reduced = total_data_for_reductions.drop_duplicates(subset=['Protein names'])\n",
    "total_data_reduced.shape\n",
    "\n",
    "names_reduced = total_data_reduced['Corona']\n",
    "features_reduced = total_data_reduced.drop('Corona', axis=1)\n",
    "reduced_protein_names = features_reduced['Protein names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use this space to import test data ### \n",
    "\n",
    "\n",
    "#features_test = pd.read_excel(\"data/\"+'proteins_selected_for_testing_complete_updated.xlsx', header=0, index_col = 0)\n",
    "features_test = pd.read_excel(\"data/\"+'netsurfp_2_proteins_selected_for_testing_processed_updated.xlsx', header=0, index_col = 0)\n",
    "\n",
    "# uncomment below for large verification runs with labels\n",
    "\n",
    "# features_test = pd.read_excel(\"data/\"+'pnp_csf_features_names_biopy_gravy.xlsx', header=0, index_col = 0)\n",
    "# y_test_test = features_test['Corona'].copy()\n",
    "# features_test = features_test.drop(['Corona'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if were not going to be using NETSURFP \n",
    "\n",
    "#features_for_prediction = pd.read_excel(\"data/\"+'proteins_selected_for_testing_complete.xlsx', header=0, index_col = 0)\n",
    "features_for_prediction = pd.read_excel(\"data/\"+'netsurfp_2_proteins_selected_for_testing_processed_updated.xlsx', header=0, index_col = 0)\n",
    "\n",
    "# print(list(features_for_prediction.columns))\n",
    "# subset_features = features_merged_naive[list(features_for_prediction.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data To Make it Work Well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(174, 91)\n",
      "{'clip': False, 'copy': True, 'feature_range': (0, 1)}\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "total_data = features_merged_naive.copy()  ## for a regular netsurfp included case\n",
    "# total_data = subset_features.copy() ### for a subset case --- use this one\n",
    "total_data = total_data.fillna(0)\n",
    "total_data_with_names = total_data.copy()\n",
    "total_data = total_data.drop(['Protein names', 'mass' ], axis=1)\n",
    "scaler = scaler.fit(total_data)\n",
    "scaled_df = pd.DataFrame(scaler.transform(total_data), columns=total_data.columns)\n",
    "#scaled_df.to_csv(\"02-intermediate_results/scaled_full_data.csv\")\n",
    "#print(scaled_df.shape)\n",
    "\n",
    "scaled_df_phase = scaled_df.copy()\n",
    "\n",
    "# Add the \"phase_plasma\" column back to the scaled dataframe\n",
    "scaled_df_phase['phase_plasma'] = features_merged['phase_plasma'].copy()\n",
    "\n",
    "plasma_data = scaled_df_phase[scaled_df_phase.phase_plasma==1]\n",
    "plasma_data = plasma_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_plasma = plasma_data #pd.DataFrame(scaler.transform(plasma_data), columns=plasma_data.columns)\n",
    "\n",
    "csf_data = scaled_df_phase[scaled_df_phase.phase_plasma==0]\n",
    "csf_data = csf_data.drop(['phase_plasma'], axis=1)\n",
    "scaled_df_csf = csf_data #pd.DataFrame(scaler.transform(csf_data), columns=csf_data.columns)\n",
    "\n",
    "### UNCOMMENT this section for a REGULAR RUN\n",
    "#features = features_merged_naive.copy()  # change the dataframe that you want to use here\n",
    "features_test = features_test.fillna(0)\n",
    "features_test_names = features_test.copy()\n",
    "\n",
    "#print(features_test.axes)\n",
    "#features_test = features_test.drop(['Protein names', 'mass'], axis=1) #,'entry'\n",
    "\"\"\"scaled_test_df = pd.DataFrame(scaler.transform(features_test), columns=features_test.columns)\"\"\"\n",
    "print(scaler.get_params())\n",
    "\n",
    "# features_reduced = features_reduced.drop(['Protein names'], axis=1)\n",
    "# scaled_reduced_df = pd.DataFrame(scaler.transform(features_reduced), columns=features_reduced.columns)\n",
    "\n",
    "\n",
    "# scaled_df = scaled_df.drop(['Unnamed: 0.1'], axis=1)\n",
    "# scaled_df_phase = scaled_df_phase.drop(['Unnamed: 0.1'], axis=1)\n",
    "# scaled_test_df = scaled_test_df.drop(['Unnamed: 0.1'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m total_data_col_drop = total_data.copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# scaler = scaler.fit(features_gt6_combined)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m scaled_df_gt6 = pd.DataFrame(\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_gt6_combined\u001b[49m\u001b[43m)\u001b[49m, columns=total_data_col_drop.columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/sklearn/utils/_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/sklearn/preprocessing/_data.py:545\u001b[39m, in \u001b[36mMinMaxScaler.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    541\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    543\u001b[39m xp, _ = get_namespace(X)\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_array_api\u001b[49m\u001b[43m.\u001b[49m\u001b[43msupported_float_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    555\u001b[39m X *= \u001b[38;5;28mself\u001b[39m.scale_\n\u001b[32m    556\u001b[39m X += \u001b[38;5;28mself\u001b[39m.min_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/sklearn/utils/validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.14/site-packages/sklearn/utils/validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Unnamed: 0\n"
     ]
    }
   ],
   "source": [
    "gt6_data = pd.read_excel(\"data/\"+'pnp_plasma_features_names_biopy_gravy.xlsx', header=0, index_col=0)\n",
    "features_gt6 = gt6_data.copy()\n",
    "features_gt6 = features_gt6.drop(['Corona'], axis=1)\n",
    "names_gt6 = gt6_data['Corona'].copy()\n",
    "\n",
    "features_gt6_combined = pd.concat([features_merged_naive, features_gt6], ignore_index=True)\n",
    "names_gt6_combined = pd.concat([names_merged, names_gt6], ignore_index=True)\n",
    "features_gt6_combined = features_gt6_combined.drop(columns=['Protein names', 'mass'])\n",
    "total_data_col_drop = total_data.copy()\n",
    "\n",
    "# scaler = scaler.fit(features_gt6_combined)\n",
    "scaled_df_gt6 = pd.DataFrame(scaler.transform(features_gt6_combined), columns=total_data_col_drop.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data to be Put into classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data put into classifier and classified\n",
    "s_df = scaled_df#.drop(['Unnamed: 0.1', 'mass' ], axis=1)\n",
    "scaled_test_df = scaled_test_df#.drop(['Unnamed: 0.1', 'mass'], axis=1)\n",
    "\n",
    "df_local_features_train = s_df.copy()\n",
    "# df_local_protein_names_train = features_merged['Protein names'].copy()#reduced_protein_names.copy()\n",
    "df_local_names = names_merged.copy()\n",
    "\n",
    "df_local_features_classify = scaled_test_df.copy() #.drop(['Unnamed: 0.1'], axis=1)\n",
    "df_local_protein_names_classify = features_test_names['Protein names'] #.copy()\n",
    "\n",
    "### to keep some things kosher later\n",
    "df_local_features_train_copy = s_df.copy()\n",
    "df_local_names_copy = names_merged.copy()\n",
    "df_local_features_classify_copy = df_local_features_classify.copy()\n",
    "df_local_protein_names_classify_copy = df_local_protein_names_classify.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold_splits = 100\n",
    "predictions = pd.DataFrame()\n",
    "\n",
    "X_new = SelectKBest(f_classif, k=38).fit_transform(df_local_features_train.copy(), df_local_names.copy()) #X_train_set.values #\n",
    "df_local_features_train = pd.DataFrame(X_new.copy())#df_local_features_train.copy() #pd.DataFrame(X_new.copy()) #scaled_df.copy()\n",
    "\n",
    "rndm_ste=2016\n",
    "feature_imp = pd.DataFrame(columns=list(df_local_features_train.columns))\n",
    "first_frame = True\n",
    "correctness_frame = pd.DataFrame()\n",
    "metrics_frame = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "set_size_adjust = (scaled_df.shape[0]/scaled_df_plasma.shape[0]) *.1 # used to retain the same number of samples in the test set, replace test_size with it if using\n",
    "# #split up our data\n",
    "i = 0\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=k_fold_splits, test_size=0.1, random_state=rndm_ste)\n",
    "\n",
    "for train_index, test_index in sss.split(df_local_features_train, df_local_names): # comment our if doing cross fluid\n",
    "# for train_index, test_index in sss.split(scaled_df_plasma, names_plasma): # use for cross fluid tests, verify correct dataset placed here\n",
    "    X_train = df_local_features_train.iloc[train_index] # remove subsetting for cross fluid tests\n",
    "    X_test = df_local_features_train.iloc[test_index] # change dataframe for cross fluid tests\n",
    "    y_train = df_local_names.iloc[train_index] # remove subsetting for cross fluid tests\n",
    "    y_test = df_local_names.iloc[test_index] # change dataframe for cross fluid tests\n",
    "\n",
    "\n",
    "     \n",
    "    # Create and Train\n",
    "    rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "     n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700) \n",
    " \n",
    "    \n",
    "    sme = SMOTE(random_state=rndm_ste, sampling_strategy=0.7, n_jobs=-1, k_neighbors=12)\n",
    "    X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "    # X_train_oversampled, y_train_oversampled = X_train, y_train # can be used to pass smote if needed for an experiment\n",
    "    rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    if first_frame:  # Initialize \n",
    "        first_frame = False  # Don't Come back Here\n",
    "        \n",
    "        datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "        \n",
    "        correctness_frame = pd.DataFrame(data=datadict)\n",
    "        correctness_frame['round'] = i\n",
    "\n",
    "        metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)), \n",
    "        'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "        \n",
    "        metrics_frame = pd.DataFrame.from_dict(data=metrics_dict,orient='index').transpose()\n",
    "        metrics_frame['Round'] = i\n",
    "\n",
    "        # can be used if you want to track prediction during shuffle split - saves in another cell\n",
    "        predictions = pd.DataFrame()\n",
    "        predictions['Protein Name'] = df_local_protein_names_classify\n",
    "        predictions['In Corona Probability'] = rfc.predict_proba(df_local_features_classify)[:, 1]\n",
    "        predictions['Round'] = i\n",
    "        predictions['Test Accuracy'] = metrics_dict['Accuracy']\n",
    "        predictions['Test Recall'] = metrics_dict['Recall']\n",
    "        predictions['Test Precision'] = metrics_dict['Precision']\n",
    "        predictions['Test AUC'] = metrics_dict['AUC']\n",
    "\n",
    "        \n",
    "    else:\n",
    "        datadict = {'true':y_test.to_numpy(), 'estimate':rfc.predict(X_test), 'probability':rfc.predict_proba(X_test)[:, 1]}\n",
    "        revolve_frame = pd.DataFrame(data=datadict)\n",
    "        revolve_frame['round'] = i\n",
    "        correctness_frame = correctness_frame.append(revolve_frame, ignore_index=True)\n",
    "\n",
    "        metrics_dict = {'AUC':metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_test, y_test), 'Recall':recall_score(y_test, rfc.predict(X_test)), \n",
    "        'Precision':precision_score(y_test, rfc.predict(X_test)), 'F1':f1_score(y_test, rfc.predict(X_test))}\n",
    "        metrics_revolve_frame = pd.DataFrame.from_dict(data=metrics_dict, orient='index').transpose()\n",
    "        metrics_revolve_frame['Round'] = i\n",
    "        metrics_frame = metrics_frame.append(metrics_revolve_frame, ignore_index=True)\n",
    "\n",
    "        # can be used if you want to track prediction during shuffle split - saves in another cell\n",
    "        pred_rev = pd.DataFrame()\n",
    "        pred_rev['Protein Name'] = df_local_protein_names_classify\n",
    "        pred_rev['In Corona Probability'] = rfc.predict_proba(df_local_features_classify)[:, 1]\n",
    "        pred_rev['Round'] = i\n",
    "        pred_rev['Test Accuracy'] = metrics_dict['Accuracy']\n",
    "        pred_rev['Test Recall'] = metrics_dict['Recall']\n",
    "        pred_rev['Test Precision'] = metrics_dict['Precision']\n",
    "        pred_rev['Test AUC'] = metrics_dict['AUC']\n",
    "\n",
    "        predictions = predictions.append(pred_rev, ignore_index=True)\n",
    "\n",
    "\n",
    "    \n",
    "    feature_imp.loc[i] = pd.Series(rfc.feature_importances_,index=list(df_local_features_train.columns))\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays results\n",
    "metrics_frame.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Revolving Predictions (if applicable)\n",
    "\n",
    "This is a feature that is not used in the manuscript "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names = []\n",
    "for i in predictions['Protein Name']:\n",
    "    if i not in unique_names:\n",
    "        unique_names.append(i)\n",
    "\n",
    "protein_avg_predictions = pd.DataFrame()\n",
    "counter = 0\n",
    "for i in unique_names:\n",
    "    avg_df = predictions[predictions['Protein Name'] == i]\n",
    "\n",
    "    if counter == 0:\n",
    "        protein_avg_predictions = pd.DataFrame([i, round(avg_df['In Corona Probability'].mean(), 3), round(confidence_interval(avg_df['In Corona Probability']), 3)], index=['Protein Name', 'Average In Corona Probability', '95 Percent Confidence Interval']).transpose()\n",
    "        \n",
    "    else:\n",
    "        pap_df = pd.DataFrame([i, round(avg_df['In Corona Probability'].mean(), 3), round(confidence_interval(avg_df['In Corona Probability']), 3)], index=['Protein Name', 'Average In Corona Probability', '95 Percent Confidence Interval']).transpose()\n",
    "        protein_avg_predictions = protein_avg_predictions.append(pap_df, ignore_index=True)\n",
    "    \n",
    "    counter += 1\n",
    "\n",
    "protein_avg_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct_correct = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    subset = correctness_frame[correctness_frame.probability>= i *.1]\n",
    "    subset = subset[subset.probability <(i+1)*.1]\n",
    "\n",
    "#     subset = correctness_frame[correctness_frame.probability>=i]\n",
    "    subset['correct'] = subset['true'] == subset['estimate']\n",
    "    pct_correct.append(subset.correct.sum() / subset.shape[0])\n",
    "    \n",
    "bar_names = ['[' + str(np.around((i-1)*.1, decimals=1)) + ', ' + str(np.around((i)*.1, decimals=1)) +')'  for i in range(1,11)]\n",
    "print(pct_correct, bar_names)#, steps)\n",
    "#subset\n",
    "fig= plt.figure(figsize=(10,5))\n",
    "sns.barplot(x=bar_names, y=pct_correct, ci=None)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_probability_accuracy = pd.DataFrame([pct_correct], columns=bar_names)\n",
    "metrics_frame = metrics_frame.append(pd.DataFrame({\"AUC\":[metrics_frame.AUC.mean(), confidence_interval(metrics_frame.AUC)], \"Accuracy\":[metrics_frame.Accuracy.mean(), confidence_interval(metrics_frame.Accuracy)], \"Precision\":[metrics_frame.Precision.mean(), confidence_interval(metrics_frame.Precision)],'Round':['Average', '.95 CI'], 'Recall':[metrics_frame.Recall.mean(), confidence_interval(metrics_frame.Recall)], 'F1':[metrics_frame.F1.mean(), confidence_interval(metrics_frame.F1)]}), ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Predictions Using the Entire Saved Dataset\n",
    "\n",
    "Ensure that you are using the right k values and data files here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_total = df_local_features_train_copy.copy() #df_local_features_train #df_local_features_train_copy\n",
    "y_train_total = df_local_names #df_local_names#df_local_names_copy\n",
    "rndm_ste = 2016\n",
    "k_best = SelectKBest(f_classif, k=38)\n",
    "fit = k_best.fit(X_train_total, y_train_total)\n",
    "X_new = fit.transform(X_train_total)\n",
    " #X_train_set.values #\n",
    "X_train_total = pd.DataFrame(X_new.copy())\n",
    "\n",
    "rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "     n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700)   \n",
    "sme = SMOTE(random_state=2016, sampling_strategy=0.7, n_jobs=-1, k_neighbors=12)\n",
    "X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train_total, y_train_total)\n",
    "rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "\n",
    "total_train_test = pd.DataFrame()\n",
    "total_train_test['Protein Name'] = df_local_protein_names_classify_copy\n",
    "# pd.DataFrame(fit.transform(df_local_features_classify_copy))\n",
    "# total_train_test['In Corona Probability'] = rfc.predict_proba(df_local_features_classify_copy)[:, 1]\n",
    "\n",
    "print(len(k_best.get_support()), df_local_features_train_copy.shape)\n",
    "total_train_test['In Corona Probability'] = rfc.predict_proba(pd.DataFrame(df_local_features_classify_copy.loc[:,k_best.get_support()]))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print weights for table s2\n",
    "pd.Series(rfc.feature_importances_,index=list(df_local_features_train_copy.columns)).sort_values(ascending=False).to_excel('revisions_data_2/table_s2_weights.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for full dataset testing scoire\n",
    "y_test_score = y_test_test \n",
    "X_score = df_local_features_classify_copy.loc[:,k_best.get_support()]\n",
    "\n",
    "results_dict = {'AUC':metrics.roc_auc_score(y_test_score, rfc.predict_proba(X_score)[:, 1]),\n",
    "        'Accuracy':rfc.score(X_score, y_test_score), 'Recall':recall_score(y_test_score, rfc.predict(X_score)), \n",
    "        'Precision':precision_score(y_test_score, rfc.predict(X_score)), 'F1':f1_score(y_test_score, rfc.predict(X_score))}\n",
    "\n",
    "pprint(results_dict)\n",
    "count_proxy  = total_train_test.copy()\n",
    "count_proxy['In Corona'] = count_proxy['In Corona Probability'] >= 0.5 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display results\n",
    "total_train_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writes all Prediction Data (Including Revolving Predictions Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "writer = pd.ExcelWriter('2021_08_08_predictions_selected_proteins_gt6.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# Write each dataframe to a different worksheet.\n",
    "\n",
    "predictions.to_excel(writer, sheet_name='Round Based Prediction')\n",
    "metrics_frame.to_excel(writer, sheet_name='Classifier Round Metrics')\n",
    "protein_avg_predictions.to_excel(writer, sheet_name='Protein Average Predictions')\n",
    "overall_probability_accuracy.to_excel(writer, sheet_name='Overall Probability Accuracy')\n",
    "total_train_test.to_excel(writer, sheet_name='Total Set Used in Prediction')\n",
    "\n",
    "\n",
    "# Close the Pandas Excel writer and output the Excel file.\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_proxy  = total_train_test.copy()\n",
    "count_proxy['In Corona'] = count_proxy['In Corona Probability'] >= 0.5 \n",
    "print(count_proxy['In Corona'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Add In Importance Test (Can take a very long time to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_readouts ={}\n",
    "first_loop = True\n",
    "first_feat=True\n",
    "kselect_params={}\n",
    "trials = 100\n",
    "\n",
    "\n",
    "rndm_ste=2016\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=trials, test_size=0.1, random_state=rndm_ste)\n",
    "\n",
    "feature_imp = pd.DataFrame(columns=list(df_local_features_train_copy.columns))\n",
    "for k_feat in range(45, 51, 1):   \n",
    "    i = 0\n",
    "\n",
    "    k_best = SelectKBest(f_classif, k=k_feat)\n",
    "    fit = k_best.fit(df_local_features_train_copy.copy(), df_local_names.copy())\n",
    "    X_new = fit.transform(df_local_features_train_copy.copy()) #X_train_set.values #\n",
    "    df_local_features_train = pd.DataFrame(X_new.copy())\n",
    "    \n",
    "    init_scores_rfc = True\n",
    "    first_roc_rfc = True\n",
    "    # kselect_params[str(k_feat)]=k_best.get_support()\n",
    "    for train_index, test_index in sss.split(df_local_features_train, df_local_names):\n",
    "    \n",
    "        X_train = df_local_features_train.iloc[train_index]\n",
    "        X_test = df_local_features_train.iloc[test_index]\n",
    "        y_train = df_local_names.iloc[train_index]\n",
    "        y_test = df_local_names.iloc[test_index]    \n",
    "        #### END COMMENT OUT \n",
    "        if init_scores_rfc:  # use this to record data for ROC curves -- Some may be moved to outside the loop\n",
    "            y_score_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            y_true_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            tpr_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            fpr_array_rfc = np.zeros((y_test.shape[0], trials))\n",
    "            score_rfc = np.zeros(trials)\n",
    "            score_svm = np.zeros(trials)\n",
    "            auc_data_rfc = np.zeros(trials)\n",
    "            f1_rfc = np.zeros(trials)\n",
    "            fbeta_rfc = np.zeros(trials)\n",
    "            recall_rfc = np.zeros(trials)\n",
    "            precision_rfc = np.zeros(trials)\n",
    "            log_loss_rfc = np.zeros(trials)\n",
    "            features_left = np.zeros(trials)\n",
    "            \n",
    "            init_scores_rfc = False # Don't Come Back Here\n",
    "        \n",
    "        # Create and Train\n",
    "        rfc=RandomForestClassifier(criterion='entropy', min_impurity_decrease = 0.02,  min_samples_split=2, max_depth = 10, max_features = 'sqrt',\n",
    "                                    n_jobs=-1, ccp_alpha=0.01, random_state=rndm_ste, n_estimators=700)\n",
    "                                #min_samples_split=4, min_samples_leaf= 2, max_features= 'log2', max_depth = 10)    ## max_leaf_nodes=20, \n",
    "        \n",
    "        \n",
    "        sme = SMOTE(random_state=rndm_ste, sampling_strategy=.7, n_jobs=-1, k_neighbors=12)\n",
    "        X_train_oversampled, y_train_oversampled = sme.fit_resample(X_train, y_train)\n",
    "        \n",
    "        rfc.fit(X_train_oversampled,y_train_oversampled)\n",
    "\n",
    "        # Basic Predictions\n",
    "        y_pred_test = rfc.predict(X_train_oversampled) \n",
    "        y_pred_train = rfc.predict(X_train)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Calculate Metrics\n",
    "        auc_data_rfc[i] = metrics.roc_auc_score(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "        score_rfc[i] =  rfc.score(X_test, y_test)\n",
    "\n",
    "        f1_rfc[i] = f1_score(y_test, rfc.predict(X_test))\n",
    "        fbeta_rfc[i] = fbeta_score(y_test, rfc.predict(X_test), beta=0.5)\n",
    "        recall_rfc[i] = recall_score(y_test, rfc.predict(X_test))\n",
    "        precision_rfc[i] = precision_score(y_test, rfc.predict(X_test))\n",
    "        log_loss_rfc[i] = log_loss(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "        fpr_current_list, tpr_current_list, _ = metrics.roc_curve(y_test, rfc.predict_proba(X_test)[:, 1])\n",
    "\n",
    "\n",
    "        if first_roc_rfc:  # Initialize \n",
    "            fpr_array_rfc = fpr_current_list\n",
    "            tpr_array_rfc = tpr_current_list\n",
    "            first_roc_rfc = False  # Don't Come back Here\n",
    "            \n",
    "        else:\n",
    "            fpr_array_rfc = np.concatenate((fpr_array_rfc, fpr_current_list))\n",
    "            tpr_array_rfc = np.concatenate((tpr_array_rfc, tpr_current_list))\n",
    "#         i+=1\n",
    "        column_list = list(scaled_df.columns)\n",
    "        \n",
    "        if first_feat:\n",
    "            feat_revolve = pd.DataFrame(rfc.feature_importances_,index=[column_list[i] for i in np.nonzero(k_best.get_support())[0]]).transpose()\n",
    "            feat_revolve['Features'] = k_feat\n",
    "            feature_imp = feat_revolve.copy()\n",
    "            first_feat = False\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            feat_revolve = pd.DataFrame(rfc.feature_importances_,index=[column_list[i] for i in np.nonzero(k_best.get_support())[0]]).transpose()\n",
    "            feat_revolve['Features'] = k_feat\n",
    "            feature_imp = feature_imp.append(feat_revolve.copy(), ignore_index=True)\n",
    "\n",
    "        kselect_params[str(k_feat)]=k_best.get_support()\n",
    "        \n",
    "        i+=1\n",
    "\n",
    "    k_readouts[str(k_feat)] = {'Accuracy rfc': score_rfc.mean(),\n",
    "                               'Accuracy rfc ci': confidence_interval(score_rfc),\n",
    "                               'ROC Score rfc': auc_data_rfc.mean(), \n",
    "                               'ROC Score rfc ci': confidence_interval(auc_data_rfc),\n",
    "                               'Precision rfc': precision_rfc.mean(),\n",
    "                               'Precision rfc ci': confidence_interval(precision_rfc),\n",
    "                               'Recall rfc': recall_rfc.mean(),\n",
    "                               'Recall rfc ci': confidence_interval(recall_rfc),\n",
    "                               'Accuracy svm': score_svm.mean(),\n",
    "                               \"Accuracy svm ci\" : confidence_interval(score_svm)}\n",
    "    print(f'K Criteria: {k_feat}\\nAccuracy: {score_rfc.mean():.03f} +/- {confidence_interval(score_rfc):.03f}\\nROC Score: {auc_data_rfc.mean():.03f} +/- {confidence_interval(auc_data_rfc):.03f} \\n Precision: {precision_rfc.mean():.03f} \\nRecall: {recall_rfc.mean():.03f}') #Features Left: {features_left.mean():.1f} +/- {confidence_interval(features_left):.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_key = True\n",
    "for key in k_readouts.keys():\n",
    "    \n",
    "    if first_key:\n",
    "        k_feats_data = pd.DataFrame.from_dict(data=k_readouts[key],orient='index').transpose()\n",
    "        k_feats_data['K Features'] = int(key)\n",
    "        first_key=False\n",
    "        \n",
    "    else: \n",
    "        k_feats_data_rotating = pd.DataFrame.from_dict(data=k_readouts[key],orient='index').transpose()\n",
    "        k_feats_data_rotating['K Features'] = int(key)\n",
    "        k_feats_data = k_feats_data.append(k_feats_data_rotating, ignore_index=True)\n",
    "        \n",
    "k_feats_data = k_feats_data.set_index('K Features')\n",
    "k_feats_data.to_excel('data_for_feature_add_in_figure_example.xlsx')        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## prints features in order of importance (use all features for best results)\n",
    "first_pass = True\n",
    "for key, value in kselect_params.items():\n",
    "    if first_pass:\n",
    "        print(key, list(df_local_features_train_copy.loc[:,value].columns)[0])\n",
    "        prev_list = list(df_local_features_train_copy.loc[:,value].columns)\n",
    "        first_pass = False\n",
    "    else: \n",
    "        for i in list(df_local_features_train_copy.loc[:,value].columns):\n",
    "            if i not in prev_list:\n",
    "                print(key, i)\n",
    "                prev_list = list(df_local_features_train_copy.loc[:,value].columns)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
